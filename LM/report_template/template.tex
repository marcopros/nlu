\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2021}

% Put the lab number of the corresponding exercise
\title{Language Modelling - Lab 4}
\name{Marco Prosperi (257857)}

\address{
  University of Trento}
\email{marco.prosperi@studenti.unitn.it}

\begin{document}

\maketitle
%
%Dear students, \\
%here you can find a complete description of the sections that you need to write for the mini-report. You have to write a mini-report of \textbf{max 1 page (references, tables and images are excluded from the count)} for each last exercise of labs 4 (LM) and 5 (NLU). \textbf{Reports longer than 1 page will not be checked.} The purpose of this is to give you a way to report cleanly the results and give you space to describe what you have done and/or the originality that you have added to the exercise.
%\\
%\textbf{If you did part A only, you have to just report the results in a table with a small description.}
%
\section{Introduction}
The project aimed to improve the baseline RNN performance by incrementally adding 
features and dynamically adjusting hyperparameters (learning rate, hidden units,
embedding size). The model was trained on the Penn Treebank dataset, using perplexity
as the main evaluation metric. In the second part, advanced techniques \cite{merityRegOpt} were 
incorporated to further enhance the language model, including Weight Tying 
to reduce parameters, Variational Dropout (excluding DropConnect) for 
regularization, and Averaged Stochastic Gradient Descent (AvSGD) for better
optimization. These improvements aimed to enhance generalization and 
convergence while maintaining low perplexity (less than 250). 

\section{Implementation details}

The first part of the project involved implementing a basic RNN model using PyTorch, 
based on the code provided by the teaching assistant at the Lab Repository \cite{nlu-labs-unitn}.
 I started by integrating the functions, model, and dataset from the Jupyter notebook into my own 
 repository. Initially, the RNN model was trained on the Penn Treebank dataset, yielding a high 
 perplexity that did not meet the target of 250. To address this, I experimented with various 
 hyperparameters, including the learning rate, hidden units, and embedding size, eventually 
 achieving an acceptable perplexity. Subsequently, I developed an LSTM model, which significantly 
 lowered the perplexity to 137.31. To further enhance performance, I incorporated two dropout layers,
  resulting in better model accuracy. Additionally, the adoption of AdamW, a weight decay optimizer, 
  further reduced the perplexity to 109.43. Given the consistent performance improvements, I retained
   these regularization techniques for the second part of the project.


In the second phase, I focused on implementing advanced techniques
 to further optimize the language model. The first enhancement was Weight Tying,
  which decreased the modelâ€™s parameter count and improved generalization. I also 
  introduced Variational Dropout \cite{gal2016theoreticallygroundedapplicationdropout}, 
  a method specifically designed for recurrent neural networks that applies a fixed dropout 
  mask across the entire input sequence, enhancing temporal consistency and training stability.

Finally, I employed the Non-monotonically triggered AvSGD \cite{merityRegOpt}, which 
switches to AvSGD when SGD converges to a steady-state distribution. The technique can be
 activated when model performance plateaus, as recommended by the authors: AvSGD is initiated 
 when the validation metric does not improve over multiple cycles. A practical implementation 
 involves using a patience mechanism similar to early stopping; once patience is exhausted, AvSGD
  is triggered, and patience is reset to enable early termination. Alternatively, the switch 
  can occur when validation loss exceeds the minimum observed within a non-monotonic window, a 
  strategy inspired by the official Salesforce Research repository \cite{salesforce-repo}.

\section{Results}
Add tables and explain how you evaluated your model. Tables and images of plots or confusion matrices do not count in the page limit.
The results of my experiments are summarized in Table \ref{tab:results}. 
\newpage

\begin{table}[h!]
  \centering
  \begin{tabular}{l p{1 cm} p{1 cm} p{1 cm} p{1 cm}}
      \midrule
      \textbf{Model} & \textbf{PPL} & \textbf{LR} & \textbf{Hidden} & \textbf{Emb} \\
      \midrule
      \multicolumn{5}{c}{\textbf{Part A}} \\
      RNN                     & 173.22 & 0.1    & 100 & 100 \\
      LSTM                    & 137.31 & 2      & 300 & 300 \\
      LSTM + Dropout Layers      & 123.14 & 2      & 300 & 300 \\
      LSTM + Dropout Layers + AdamW & 109.43 & 0.001 & 400 & 400 \\
      \midrule
      \multicolumn{5}{c}{\textbf{Part B}} \\
      \midrule
      LSTM + Weight Tying + Var Dropout + AvSDG & 90.56 & 2 & 400 & 400 \\
      \bottomrule
  \end{tabular}
  \caption{Perplexity and hyperparameters of the best models.}
  \label{tab:results}
\end{table}


\bibliographystyle{IEEEtran}

\bibliography{mybib}

\end{document}
