\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2021}

% Put the lab number of the corresponding exercise
\title{Slot filling and Intent classification  - Lab 5}
\name{Marco Prosperi (257857)}

\address{
  University of Trento}
\email{marco.prosperi@studenti.unitn.it}

\begin{document}

\maketitle
%
%Dear students, \\
%here you can find a complete description of the sections that you need to write for the mini-report. You have to write a mini-report of \textbf{max 1 page (references, tables and images are excluded from the count)} for each last exercise of labs 4 (LM) and 5 (NLU). \textbf{Reports longer than 1 page will not be checked.} The purpose of this is to give you a way to report cleanly the results and give you space to describe what you have done and/or the originality that you have added to the exercise.
%\\
%\textbf{If you did part A only, you have to just report the results in a table with a small description.}
%
\section{Introduction}
The first part of the NLU project involved enhancing the IAS model for intent 
classification and slot filling on the ATIS dataset \cite{nlu-labs-unitn}, evaluated using F1-score and
 accuracy. Two key features were added: a BiLSTM layer, which processes input 
 sequences bidirectionally to better capture contextual information, and dropout 
 layers to prevent overfitting by randomly deactivating neurons during training.
 These changes aimed to improve the model’s generalization on unseen data. In the 
 second part, the LSTM architecture was replaced with a BERT-based model. This 
 involved fine-tuning and handling sub-tokenization, leveraging BERT’s contextual
 embeddings to boost performance in both tasks.
\section{Implementation details}
For the first part of the project, I've added incremental changes to the base IAS model.
First, I added a bidirectional layer to the LSTM architecture, allowing the model to
process the input sequence in both forward and backward directions. 
This change was implemented by setting the bidirectional parameter to True in the LSTM layer and by multiplying the output
 by 2 because it concatenates the forward and backward outputs.
 Dropout layers were also introduced after the embedding and LSTM layers to reduce overfitting and improve generalization.

 In the second part, the LSTM architecture was replaced with a BERT-based model, following the joint learning approach described by Chen et al.~\cite{chen2019bertjointintentclassification}.
 The architecture consists of a pre-trained BERT encoder, with two task-specific heads: one for intent classification (using the [CLS] token) and one for slot filling (using the contextualized embeddings of each token).
 A key challenge was aligning slot labels with BERT’s sub-tokenization (WordPiece).
 For each word split into sub-tokens, only the first sub-token was assigned the original corresponding slot label, while the others were masked out from the loss computation using the ignore index (-100).
 This ensured that the slot filling loss was computed only on valid positions.
 
 The implemented model, named \textit{JointBERT}, is based on the \texttt{BertPreTrainedModel} class from the Transformers library.
 It comprises a BERT encoder, followed by two linear layers: one dedicated to intent classification and the other to slot filling.
 A dropout layer is applied before both classification heads to reduce overfitting.
 During the forward pass, the model first processes the input through BERT to obtain the last hidden states and the pooled [CLS] output.
 The pooled [CLS] embedding is fed to the intent classifier, while the full sequence of hidden states is fed to the slot classifier.
 The final loss is computed as the sum of the intent classification loss and the slot filling loss, where only the active tokens (those corresponding to valid input words) contribute to the slot loss computation.
 
 The model was fine-tuned end-to-end on the ATIS dataset using the AdamW optimizer with a learning rate of $2 \times 10^{-5}$, a batch size of 16, and early stopping based on the slot F1-score on the validation set.
 The best model for each run was saved in a dedicated folder, and evaluation was performed on the official ATIS test set.
 Additionally, experiments were conducted using both the BERT base and large versions, with the large model achieving superior results at the cost of longer training time.


\section{Results}
The IAS Model was trained for 200 epoch over 5 runs, with early stopping based on the slot F1-score on the validation set.
The learning rate was set to 0.0001, with a batch size of 128 for training and 64 for validation and testing. 
Results of the first part of the project are shown in Table \ref{tab:partAresults}.
\begin{table}[h!]
  \centering
  \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{Mean Slot F1} & \textbf{Mean Intent Acc.} \\
    \midrule
    Baseline IAS & 0.925 & 0.934 \\
    IAS + Bidirectionality & 0.928 & 0.936 \\
    IAS + Bidirectionality + Dropout & 0.94 & 0.941 \\ 
    \bottomrule
  \end{tabular}
  \caption{Results for Part A: mean slot F1 and intent accuracy for each model variant.}
  \label{tab:partAresults}
\end{table}

The BERT-based joint model achieved strong results on the ATIS dataset. Over three runs, for the bert-base-uncased model, the average slot filling F1-score was \textbf{ 0.951}, with a standard deviation of \textbf{0.002}, and the average intent classification accuracy
 was \textbf{0.969}, with a standard deviation of \textbf{0.006}. The bert-large-uncased model performed even better, achieving an average slot filling F1-score of \textbf{0.955} (std: \textbf{0.004}) and an average intent classification accuracy of \textbf{0.978} (std: \textbf{0.004}). 
These results confirm the effectiveness of BERT for joint NLU tasks, significantly outperforming the LSTM baseline in both metrics.

\begin{table}[h!]
  \centering
  \begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{Slot F1} & \textbf{Intent Acc.} \\
    \midrule
    JointBERT (base) & 0.951 & 0.969 \\
    JointBERT (large)   & 0.955 & 0.978 \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of BERT-based models on ATIS.}
  \label{tab:results}
\end{table}

For the BERT-based model, the training was performed for 3 epochs with a batch size of 16 for 
training, 32 for testing and validation, and a learning rate of $2 \times 10^{-5}$.

\bibliographystyle{IEEEtran}

\bibliography{mybib}

\end{document}
